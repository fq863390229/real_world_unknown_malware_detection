import numpy as np 
from keras.src.layers import Input, Dense  ,Lambda
from keras.src.models import Model
from sklearn.model_selection import train_test_split 
from keras.src.optimizers import Adam 
from torch.utils.data import TensorDataset, DataLoader, random_split 
from keras.src import backend as K
import tensorflow as tf
from keras import layers
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import torch.optim as optim 
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
def encoder_and_decoder(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2 ,random_state=42)
    input_dim = X.shape[1]  
 
    encoding_dim = 768  

    input_layer = Input(shape=(input_dim,))  
    encoded = Dense(encoding_dim, activation='relu')(input_layer)  
    
    decoded = Dense(input_dim, activation='sigmoid')(encoded)  

    autoencoder = Model(input_layer, decoded)  

    encoder = Model(input_layer, encoded)  
    
  
    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')  
  
    autoencoder.fit(X_train, X_train, epochs=1000, batch_size=256, shuffle=True, validation_data=(X_test, X_test))
    
    encoded_train_data = encoder.predict(X_train)
    encoded_test_data = encoder.predict(X_test)
    encoded_data = np.concatenate((encoded_train_data, encoded_test_data))
    y_all = np.concatenate((y_train, y_test)) 
    np.save('./auto_encoder_dataset/androzoo_known_data.npy', encoded_data)
    np.save('./auto_encoder_dataset/androzoo_known_labels.npy', y_all)
    
    
    return encoded_data,y_all

class VAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super(VAE, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc21 = nn.Linear(hidden_dim, latent_dim)
        self.fc22 = nn.Linear(hidden_dim, latent_dim)
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std

    def decode(self, z):
        h3 = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 768))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar
    

  
 
class StackedAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dims):
        super(StackedAutoencoder, self).__init__()
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        

        self.encoder1 = nn.Linear(input_dim, hidden_dims[0])
        self.encoder2 = nn.Linear(hidden_dims[0], hidden_dims[1])
        self.encoder3 = nn.Linear(hidden_dims[1], hidden_dims[2])
        
        self.decoder3 = nn.Linear(hidden_dims[2], hidden_dims[1])
        self.decoder2 = nn.Linear(hidden_dims[1], hidden_dims[0])
        self.decoder1 = nn.Linear(hidden_dims[0], input_dim)
        #self.decoder1 = nn.Linear(hidden_dims[0], 128)

        

        self.activation = nn.GELU()
        
    def encoder(self, x):
        z1 = self.activation(self.encoder1(x))
        #z2 = self.activation(self.encoder2(z1))
        #z3 = self.activation(self.encoder3(z2))
        return z1
    
    def decoder(self, z):
       # xhat3 = self.activation(self.decoder3(z))
       # xhat2 = self.activation(self.decoder2(xhat3))
        #xhat1 = self.decoder1(xhat2)
        xhat1 = self.decoder1(z)
        return xhat1
    
    def forward(self, x):
        z = self.encoder(x)
        xhat = self.decoder(z)
        return xhat
    
    def get_encoder_output(self, x):
        return self.encoder(x)



def VAE_train(X ,y):
    #encoder_and_decoder(X, y)
    X_train_tensor = torch.from_numpy(X).float() 
    y_train_tensor = torch.from_numpy(y).long() 
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor) 
    
    train_size = int(0.8 * len(train_dataset))  
    test_size = len(train_dataset) - train_size 
    
    train_dataset, test_dataset = random_split(train_dataset, [train_size, test_size])
     
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    input_dim = 768
    hidden_dim = 768
    last_dim = 768
    loss_fn = nn.MSELoss(reduction="sum")
    epoch = 20
    model = VAE(input_dim=input_dim, hidden_dim=hidden_dim,latent_dim=last_dim).to(device)
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        optimizer.zero_grad()
        data = data.to(device)
        recon_batch, mu, logvar = model(data)
        loss = loss_fn(data,recon_batch)
        loss_KL =torch.pow(mu,2)+torch.exp(logvar) - logvar-1
        loss = loss+ 0.5 * torch.sum(loss_KL)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()
    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))
    torch.save(model, "./model/cic_VAE.pth")

def Stack_train(X, y):
    epochs = 20
    X_train_tensor = torch.from_numpy(X).float() 
    y_train_tensor = torch.from_numpy(y).long() 
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor) 
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = StackedAutoencoder(768,[32,32,32]).to(device=device)
    criterion = nn.MSELoss() 
    optimizer = optim.Adam(model.parameters(), lr=0.001) 
    
    for epoch in range(epochs):
        for data in train_loader:
            inputs, target = data
            #inputs= inputs.view(inputs.size(0), -1)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, inputs)
            loss.backward()
            optimizer.step()
        

        print('Epoch [{}/{}], Loss: {:f}'.format(epoch+1,epochs, loss.item()))
    torch.save(model, "./auto_encoder_model/SAE_cic.pth")



if __name__ == '__main__':
    #X = np.load('./feature_vector and labels/androzoo_feature_sim.npy')  
    #y = np.load('./feature_vector and labels/androzoo_label_sim.npy')
    
    X = np.load('./feature_vector and labels/cic_feature_feature_similar.npy')  
    y = np.load('./feature_vector and labels/cic_label_feature_similar.npy')
    
    #encoded_data = Stack_train(X, y)
    VAE_train(X,y)
    #Stack_train(X,y)
    #np.save('./auto_encoder_dataset/androzoo_known_data_S.npy', encoded_data)
    #np.save('./auto_encoder_dataset/androzoo_known_labels_.npy', y)
   
    
    