import torch  
import torch.nn as nn  
import torch.nn.functional as F
import numpy as np
from torch.utils.data import TensorDataset, DataLoader, random_split 
import torch.optim as optim
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  ,classification_report  
import logging

class SelfAttention(nn.Module):  
    def __init__(self, embed_size):  
        super(SelfAttention, self).__init__()  
        self.query = nn.Linear(embed_size, embed_size)  
        self.key = nn.Linear(embed_size, embed_size)  
        self.value = nn.Linear(embed_size, embed_size)  
        self.gamma = nn.Parameter(torch.zeros(1))  
  
    def forward(self, x):   
    
        query = self.query(x).unsqueeze(1)   
        key = self.key(x).unsqueeze(1)    
        value = self.value(x).unsqueeze(1) 
     
        attention_scores = torch.matmul(query, key.transpose(2, 1))    
        attention_scores = attention_scores.squeeze(1).squeeze(1)  
    

        attention_weights = F.softmax(attention_scores, dim=0)  

        attention_weights = attention_weights.unsqueeze(1) 

        output = torch.matmul(attention_weights, value).squeeze(1)  
    
        output = self.gamma * output + x   
    
        return output
  
class LSTMWithAttention(nn.Module):  
    def __init__(self, input_size, hidden_size, output_size, num_layers):  
        super(LSTMWithAttention, self).__init__()  
        self.hidden_size = hidden_size  
        self.num_layers = num_layers  
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)  
        self.attention = SelfAttention(hidden_size)  
        self.fc = nn.Linear(hidden_size, output_size)  
  
    def forward(self, x):  
        h0 = torch.zeros(self.num_layers, self.hidden_size).to(x.device)  
        c0 = torch.zeros(self.num_layers, self.hidden_size).to(x.device)  
  
        out, _ = self.lstm(x, (h0, c0))  
  
        out = self.attention(out)  
  
        out = out[:, -1, :]  
  
   
        out = self.fc(out)  
  
        return out

class RNN(nn.Module): 
     
    def __init__(self, input_size, hidden_size, output_size, num_layers):  
        super(RNN, self).__init__()  
        self.hidden_size = hidden_size  
        self.num_layers = num_layers  
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)  
        self.fc = nn.Linear(hidden_size, output_size)  
  
    def forward(self, x):  
        h0 = torch.zeros(self.num_layers, self.hidden_size).to(x.device)  
           
        out, _ = self.rnn(x, h0)  
           
        out = self.fc(out)  
        return out  
    
class LSTM(nn.Module):  
       
    def __init__(self, input_size, hidden_size, output_size, num_layers):    
        super(LSTM, self).__init__()    
        self.hidden_size = hidden_size    
        self.num_layers = num_layers    
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)    
        self.fc = nn.Linear(hidden_size, output_size)    
    
    def forward(self, x):     
        h0 = torch.zeros(self.num_layers, self.hidden_size).to(x.device)    
        c0 = torch.zeros(self.num_layers, self.hidden_size).to(x.device)  
          
        out, _ = self.lstm(x, (h0, c0))    
        
        out = self.fc(out)    
        return out
    
class BiLSTM(nn.Module):  
       
    def __init__(self, input_size, hidden_size, output_size, num_layers):    
        super(BiLSTM, self).__init__()    
        self.hidden_size = hidden_size    
        self.num_layers = num_layers    
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)    

        self.fc = nn.Linear(hidden_size * 2, output_size)    
    
    def forward(self, x):    
 
        h0 = torch.zeros(self.num_layers * 2,  self.hidden_size).to(x.device) 
        c0 = torch.zeros(self.num_layers * 2,  self.hidden_size).to(x.device)   
          

        out, _ = self.lstm(x, (h0, c0))    
          
        out = self.fc(out) 
        return out

class CNN(nn.Module):  
    def __init__(self):  
        super(CNN, self).__init__()  
  
       
        self.conv = nn.Conv1d(in_channels=1,  kernel_size=2, padding=1,out_channels=1)  
        self.fc = nn.Linear(769, 2)  
  
    def forward(self, x):  


        x = self.conv(x)  

        out = self.fc(x)  
  
        return out  

class RCNN(nn.Module):  
    def __init__(self,input_size, hidden_size, output_size, num_layers):  
        super(RCNN, self).__init__()  
        self.hidden_size = hidden_size  
        self.num_layers = num_layers  
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) 
        self.conv = nn.Conv1d(in_channels=1, kernel_size=2, padding=1, out_channels=1)   
          
        
        self.fc = nn.Linear(129, 2)    
      
          
    def forward(self, x):  
        h0 = torch.zeros(self.num_layers, self.hidden_size).to(x.device)  
           
        out, _ = self.rnn(x, h0)  
           
        x = self.conv(out)  
        out = self.fc(x)   
          
        return out 

if __name__ == '__main__':
    logging.basicConfig(filename='./log/train.txt',  
                    level=logging.INFO,  
                    format='%(asctime)s - %(levelname)s - %(message)s')
    logging.info(f"\n----------------------------------start DL training----------------------------------------------")
    logging.info(f"\n----------------------------------dataset:androzoo_known----------------------------------------------")
    logging.info(f"\n----------------------------------model:lstm---------------------------------------------")
    X = np.load('./feature_vector and labels/androzoo_feature.npy')  
    y = np.load('./feature_vector and labels/androzoo_label.npy')
 
    
    
    X_train_tensor = torch.from_numpy(X).float() 

    y_train_tensor = torch.from_numpy(y).long() 
    
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor) 
    
    train_size = int(0.99 * len(train_dataset))  
    test_size = len(train_dataset) - train_size 
    
    train_dataset, test_dataset = random_split(train_dataset, [train_size, test_size])
     
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    
    input_size = 768
    hidden_size = 128    
    num_layers = 2     
    num_classes = 2     
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)


    
    criterion = nn.CrossEntropyLoss()  
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    num_epochs = 150  
    for epoch in range(num_epochs):  
        model.train()  
        running_loss = 0.0  
        corrects = 0  
        total = 0  
      
        for inputs, labels in train_loader:  
            inputs, labels = inputs.to(device), labels.to(device) 
            optimizer.zero_grad()   
            outputs = model(inputs)  
            loss = criterion(outputs, labels)  
            loss.backward()   
            optimizer.step()  
           
            running_loss += loss.item() * inputs.size(0)  
            _, preds = torch.max(outputs, 1)  
            corrects += torch.sum(preds == labels.data)  
            total += labels.size(0)  
      
    
        epoch_loss = running_loss / total   
        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}') 
        logging.info(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}') 

        model.eval()    
        val_loss = 0.0  
        corrects = 0  
        total = 0  
        y_true = []  
        y_pred = []  
  
        with torch.no_grad():  
            for inputs, labels in test_loader:  
                inputs, labels = inputs.to(device), labels.to(device) 
                outputs = model(inputs)    
                loss = criterion(outputs, labels)    
                val_loss += loss.item() * inputs.size(0)  
                _, predicted = torch.max(outputs, 1)   
                corrects += torch.sum(predicted == labels.data)  
                total += labels.size(0)  
                y_true.extend(labels.cpu().numpy())  
                y_pred.extend(predicted.cpu().numpy()) 
  
 
        val_loss = val_loss / total  
        print(f'Validation Loss: {val_loss:.4f}')
        
        if epoch+1 == num_epochs:

            accuracy = accuracy_score(y_true, y_pred)  
            precision = precision_score(y_true, y_pred)  
            recall = recall_score(y_true, y_pred)  
            f1 = f1_score(y_true, y_pred) 
            print(classification_report(y_true, y_pred)) 
            print(f"Accuracy:{accuracy}")
            print(f"Precision:{precision}")
            print(f"recall:{recall}")
            print(f"f1:{f1}")
        
            logging.info(classification_report(y_true, y_pred))
            logging.info(f"Accuracy:{accuracy}")
            logging.info(f"Precision:{precision}")
            logging.info(f"recall:{recall}")
            logging.info(f"f1:{f1}")

        
        
        
    
    
    torch.save(model.state_dict(), f'./model/model_{num_epochs}.pth')
    

