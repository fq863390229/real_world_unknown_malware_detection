from dl_model_train import SimpleRNN
from dl_model_train import SimpleLSTM
from dl_model_train import SimpleBiLSTM
from dl_model_train import SimpleLSTMWithAttention
import torch  
import torch.nn as nn  
import torch.nn.functional as F
import numpy as np
import torch.optim as optim
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  
from torch.utils.data import TensorDataset, DataLoader, random_split 
import logging


if __name__ == '__main__':
    logging.basicConfig(filename='./log/train.txt',  
                    level=logging.INFO,  
                    format='%(asctime)s - %(levelname)s - %(message)s')
    logging.info(f"\n----------------------------------start DL testing----------------------------------------------")
    logging.info(f"\n----------------------------------dataset:androzoo----------------------------------------------")
    X_test = np.load('./realword/realworld_feature.npy')  
    y_test = np.load('./realworld/realworld_label.npy')
    print(len(X_test))

    X_test_tensor = torch.from_numpy(X_test).float() 
    y_test_tensor = torch.from_numpy(y_test).long() 
    
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor) 
    
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)
    
    input_size = 768   
    hidden_size = 128    
    num_layers = 2     
    num_classes = 2     
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = SimpleLSTM(input_size, hidden_size, num_layers, num_classes).to(device)
    
    model.load_state_dict(torch.load('model/model.pth', map_location=device)) 
    model.eval()
    
    correct = 0  
    total = 0  
    precisions = []  
    recalls = []  
    f1s = []  
  
    with torch.no_grad():  
        for data in test_loader:  
            inputs, labels = data[0].to(device), data[1].to(device)  
            outputs = model(inputs)  
            _, predicted = torch.max(outputs, 1)  
            total += labels.size(0)  
            correct += (predicted == labels).sum().item()    
            precision = precision_score(labels.cpu().numpy(), predicted.cpu().numpy())  
            recall = recall_score(labels.cpu().numpy(), predicted.cpu().numpy())  
            f1 = f1_score(labels.cpu().numpy(), predicted.cpu().numpy())  
            precisions.append(precision)  
            recalls.append(recall)  
            f1s.append(f1)  
 
    accuracy = correct / total  
    precision = np.mean(precisions)  
    recall = np.mean(recalls)  
    f1 = np.mean(f1s)  
    
    print(f"Accuracy: {accuracy}")  
    print(f"Precision: {precision}")  
    print(f"Recall: {recall}")  
    print(f"F1 Score: {f1}")
    
    logging.info(f"Accuracy:{accuracy}")
    logging.info(f"Precision:{precision}")
    logging.info(f"recall:{recall}")
    logging.info(f"f1:{f1}")